{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b314687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import openai\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0debae23",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()  # Load environment variables from .env file\n",
    "\n",
    "# Ensure the OPENAI_API_KEY is set\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if openai.api_key is None:\n",
    "    raise ValueError(\"OPENAI_API_KEY environment variable not set\")\n",
    "\n",
    "client = openai.OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a76d360",
   "metadata": {},
   "source": [
    "# Responses API - File search\n",
    "File search is a tool available in the Responses API. It enables models to retrieve information in a knowledge base of previously uploaded files through semantic and keyword search. By creating vector stores and uploading files to them, you can augment the models' inherent knowledge by giving them access to these knowledge bases or vector_stores.\n",
    "\n",
    "This is a hosted tool managed by OpenAI, meaning you don't have to implement code on our end to handle its execution. When the model decides to use it, it will automatically call the tool, retrieve information from our files, and return an output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38a5d16",
   "metadata": {},
   "source": [
    "## Step 1: Create a vector store and upload a file\n",
    "Prior to using file search with the Responses API, we need to have set up a knowledge base in a vector store and uploaded files to it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be9e247",
   "metadata": {},
   "source": [
    "**Upload the file** to the File API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a85e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep_research_blog.pdf\n",
      "file-CsaxfmQL1ciHvcQH5Vu1v8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'file-CsaxfmQL1ciHvcQH5Vu1v8'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def create_file(client, file_path):\n",
    "    if file_path.startswith(\"http://\") or file_path.startswith(\"https://\"):\n",
    "        # Download the file content from the URL\n",
    "        response = requests.get(file_path)\n",
    "        file_content = BytesIO(response.content)\n",
    "        file_name = file_path.split('/')[-1]\n",
    "        file_tuple = (file_name, file_content)\n",
    "        \n",
    "        # Create the file using the OpenAI client\n",
    "        result = client.files.create(\n",
    "            file=file_tuple,\n",
    "            purpose=\"assistants\"\n",
    "        )\n",
    "    else:\n",
    "        # Handle local file path\n",
    "        with open(file_path, \"rb\") as file_content:\n",
    "            result = client.files.create(\n",
    "                file=file_content,\n",
    "                purpose=\"assistants\"\n",
    "            )\n",
    "    print(result.id)\n",
    "    return result.id\n",
    "\n",
    "# Replace with the file path or URL\n",
    "file_id = create_file(client, \"https://cdn.openai.com/API/docs/deep_research_blog.pdf\")\n",
    "file_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf727779",
   "metadata": {},
   "source": [
    "**Create a vector store**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe2e567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vs_68482bede36481918034b42d74e508e5\n"
     ]
    }
   ],
   "source": [
    "vector_store = client.vector_stores.create(\n",
    "    name=\"knowledge_base\"\n",
    ")\n",
    "print(vector_store.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909f0a8d",
   "metadata": {},
   "source": [
    "**Add the file** to the vector store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0dfcf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VectorStoreFile(id='file-CsaxfmQL1ciHvcQH5Vu1v8', created_at=1749560326, last_error=None, object='vector_store.file', status='in_progress', usage_bytes=0, vector_store_id='vs_68482bede36481918034b42d74e508e5', attributes={}, chunking_strategy=StaticFileChunkingStrategyObject(static=StaticFileChunkingStrategy(chunk_overlap_tokens=400, max_chunk_size_tokens=800), type='static'))\n"
     ]
    }
   ],
   "source": [
    "result = client.vector_stores.files.create_and_poll(\n",
    "    vector_store_id=vector_store.id,\n",
    "    file_id=file_id\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97d5e2b",
   "metadata": {},
   "source": [
    "## Step 2: Create a model with the file search tool\n",
    "Once our knowledge base is set up, we can include the file_search tool in the list of tools available to the model, along with the list of vector stores in which to search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8680659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response(id='resp_68482dcad934819a9bc12477855b7ad706e7d7e5b249e519', created_at=1749560778.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[ResponseFileSearchToolCall(id='fs_68482dd8fd58819a937b001f19ca155b06e7d7e5b249e519', queries=['What is deep research by OpenAI?'], status='completed', type='file_search_call', results=None), ResponseOutputMessage(id='msg_68482dea2dcc819a94a791d099fcaae406e7d7e5b249e519', content=[ResponseOutputText(annotations=[AnnotationFileCitation(file_id='file-CsaxfmQL1ciHvcQH5Vu1v8', index=1518, type='file_citation', filename='deep_research_blog.pdf'), AnnotationFileCitation(file_id='file-CsaxfmQL1ciHvcQH5Vu1v8', index=1518, type='file_citation', filename='deep_research_blog.pdf'), AnnotationFileCitation(file_id='file-CsaxfmQL1ciHvcQH5Vu1v8', index=1850, type='file_citation', filename='deep_research_blog.pdf'), AnnotationFileCitation(file_id='file-CsaxfmQL1ciHvcQH5Vu1v8', index=1850, type='file_citation', filename='deep_research_blog.pdf'), AnnotationFileCitation(file_id='file-CsaxfmQL1ciHvcQH5Vu1v8', index=2078, type='file_citation', filename='deep_research_blog.pdf')], text=\"Deep Research by OpenAI is an advanced capability introduced in ChatGPT that allows for multi-step research tasks using online information. This feature functions as an agentic assistant that can synthesize large amounts of data and conduct complex research, significantly faster than traditional methods.\\n\\n### Key Features of Deep Research:\\n- **Independent Operation**: Users provide a prompt, and the model independently searches, analyzes, and synthesizes information from numerous online sources to create detailed reports.\\n- **Multi-Step Reasoning**: The model can process intricate queries, find relevant data, and adapt to new information in real-time.\\n- **Strong Documentation**: Every output includes citations and a clear summary of thought processes, facilitating easy verification of information.\\n- **Niche Information Sourcing**: It excels at uncovering less obvious facts that typically require extensive research across multiple sites.\\n  \\n### Usage:\\nUsers can access Deep Research through the ChatGPT interface by selecting the appropriate function and inputting their queries, which can be detailed and multifaceted, such as market analyses or personalized recommendations.\\n\\n### Benefits:\\n- **Efficiency**: It can automate hours of manual research, providing quicker and often more comprehensive results.\\n- **High Accuracy**: The underlying model achieves higher accuracy on various expert-level tasks compared to previous iterations, showcasing effectiveness in diverse fields from science to business.\\n\\n### Limitations:\\nWhile Deep Research enhances research capabilities, it is not without limitations:\\n- The model may sometimes hallucinate data or misinterpret sources, although at a lower frequency compared to prior versions.\\n- Current limitations also include challenges in distinguishing between credible information and rumors.\\n\\nDeep Research highlights a significant advancement toward OpenAI's broader goals in artificial general intelligence (AGI), progressively moving towards capabilities that can generate novel insights and knowledge independently.\", type='output_text', logprobs=None)], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FileSearchTool(type='file_search', vector_store_ids=['vs_68482bede36481918034b42d74e508e5'], filters=None, max_num_results=20, ranking_options=RankingOptions(ranker='auto', score_threshold=0.0))], top_p=1.0, background=False, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=ResponseUsage(input_tokens=17684, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=408, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=18092), user=None, store=True)\n"
     ]
    }
   ],
   "source": [
    "client = openai.OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=\"What is deep research by OpenAI?\", \n",
    "    tools=[{\n",
    "        \"type\": \"file_search\",\n",
    "        \"vector_store_ids\": [vector_store.id],\n",
    "    }]\n",
    ")\n",
    "print(response.output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
