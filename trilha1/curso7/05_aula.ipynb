{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "307a1899",
   "metadata": {},
   "source": [
    "# Primeiros prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee35535d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90441228",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()  # Load environment variables from .env file\n",
    "\n",
    "# Ensure the OPENAI_API_KEY is set\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if openai.api_key is None:\n",
    "    raise ValueError(\"OPENAI_API_KEY environment variable not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b29e91",
   "metadata": {},
   "source": [
    "## Utilizando chat completions - não é mais o padrão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00c715d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Criando um cliente OpenAI com a chave de API key sendo carregada direto das variáveis de ambiente (arquivo \".env\")\n",
    "#  - Isso é recomendado por questões de segurança.\n",
    "client = OpenAI()\n",
    "\n",
    "#  Uncomment the line below if you want to create a client instance with the API key explicitly\n",
    "#  client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb65214c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mensagens = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"O que é uma maçã em até cinco palavras?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "resposta = client.chat.completions.create(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    messages=mensagens,\n",
    "    max_tokens=50,\n",
    "    temperature=0.7,\n",
    "    #top_p=1.0,\n",
    "    #n=1,\n",
    "    #stop=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16369ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-Bf4LVvFww80GYfxq1Gad1d0aDSea4', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Fruto comestível, doce, nutritivo.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1749127853, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_38343a2f8f', usage=CompletionUsage(completion_tokens=11, prompt_tokens=18, total_tokens=29, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43ea9207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta: Fruto comestível, doce, nutritivo.\n"
     ]
    }
   ],
   "source": [
    "resposta_texto = resposta.choices[0].message.content.strip()\n",
    "print(f\"Resposta: {resposta_texto}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8463f56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'O que é uma maçã em até cinco palavras?'},\n",
       " {'role': 'assistant', 'content': 'Fruto comestível, doce, nutritivo.'},\n",
       " {'role': 'assistant', 'content': 'Fruto comestível, doce, nutritivo.'},\n",
       " {'role': 'user', 'content': 'E qual a sua cor típica?'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Atualizando a lista de mensagens com a resposta do assistente\n",
    "# e fazendo uma nova pergunta\n",
    "mensagens.extend([\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": resposta_texto,\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"E qual a sua cor típica?\",\n",
    "    },\n",
    "])\n",
    "mensagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c80825b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "resposta = client.chat.completions.create(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    messages=mensagens,\n",
    "    max_tokens=50,\n",
    "    temperature=0.7,\n",
    "    #top_p=1.0,\n",
    "    #n=1,\n",
    "    #stop=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5e7296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-Bf4UkTRzecGnWNKRfwqGlbkoX18Pd', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Vermelha, verde ou amarela.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1749128426, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_f12167b370', usage=CompletionUsage(completion_tokens=9, prompt_tokens=44, total_tokens=53, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41570279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta: Vermelha, verde ou amarela.\n"
     ]
    }
   ],
   "source": [
    "resposta_texto = resposta.choices[0].message.content.strip()\n",
    "print(f\"Resposta: {resposta_texto}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb5407f",
   "metadata": {},
   "source": [
    "### Explorando classe de resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b9215ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-Bf4UkTRzecGnWNKRfwqGlbkoX18Pd', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Vermelha, verde ou amarela.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1749128426, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_f12167b370', usage=CompletionUsage(completion_tokens=9, prompt_tokens=44, total_tokens=53, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "06e4c3df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Vermelha, verde ou amarela.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resposta.choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "80bd983d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content='Vermelha, verde ou amarela.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resposta.choices[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7be2e46f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': 'Vermelha, verde ou amarela.',\n",
       " 'role': 'assistant',\n",
       " 'annotations': []}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resposta.choices[0].message.model_dump(exclude_none=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be5912cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionUsage(completion_tokens=9, prompt_tokens=44, total_tokens=53, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resposta.usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f862c0",
   "metadata": {},
   "source": [
    "### Função para gerar texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "34a7dcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gera_texto(client, mensagens, model=\"gpt-4.1-nano\", max_tokens=50, temperature=0.7, top_p=1.0, n=1, stop=None):\n",
    "    resposta = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=mensagens,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        n=n,\n",
    "        stop=stop\n",
    "    )\n",
    "\n",
    "    resposta_texto = resposta.choices[0].message.content.strip()\n",
    "    print(f\"Resposta: {resposta_texto}\")\n",
    "\n",
    "    # Atualizando a lista de mensagens com a resposta do assistente\n",
    "    mensagens.append(resposta.choices[0].message.model_dump(exclude_none=True))    \n",
    "\n",
    "    return mensagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44a792be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta: 1. Fonte de fibras, melhora digestão.  \n",
      "2. Rico em antioxidantes, combate radicais livres.  \n",
      "3. Ajuda na saúde cardiovascular.\n"
     ]
    }
   ],
   "source": [
    "mensagens.append(\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Liste 3 principais benefícios de comer maçãs.\",\n",
    "    }\n",
    ")\n",
    "\n",
    "mensagens = gera_texto(client, mensagens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6a7c49b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'O que é uma maçã em até cinco palavras?'},\n",
       " {'role': 'assistant', 'content': 'Fruto comestível, doce, nutritivo.'},\n",
       " {'role': 'user', 'content': 'E qual a sua cor típica?'},\n",
       " {'role': 'user', 'content': 'Liste 3 principais benefícios de comer maçãs.'},\n",
       " {'role': 'assistant',\n",
       "  'content': '1. Fonte de fibras, melhora digestão.  \\n2. Rico em antioxidantes, combate radicais livres.  \\n3. Ajuda na saúde cardiovascular.'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mensagens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e0bcba",
   "metadata": {},
   "source": [
    "## Utilizando a Responses API - Novo PADRÃO recomendado pela OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b9626c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "client2 = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e18c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under a silver moon, a gentle unicorn and a friendly dragon soared above a sparkling lake, weaving dreams of magic and peace for all who slept below.\n"
     ]
    }
   ],
   "source": [
    "response = client2.responses.create(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    #model=\"gpt-4.1\",\n",
    "    input=\"Write a one-sentence bedtime story about a unicorn and a dragon.\",\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d689e1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ResponseOutputMessage(id='msg_6842e7590d50819998702a31402c91f60818890ee5840cac', content=[ResponseOutputText(annotations=[], text='Once upon a time, a gentle unicorn and a kind-hearted dragon became best friends and spent their days sharing magical adventures anddreams under a sky full of twinkling stars.', type='output_text', logprobs=None)], role='assistant', status='completed', type='message')]\n"
     ]
    }
   ],
   "source": [
    "print(response.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efa4873",
   "metadata": {},
   "source": [
    "### Giving instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c2b563",
   "metadata": {},
   "source": [
    "The instructions parameter gives the model high-level instructions on how it should behave while generating a response, including tone, goals, and examples of correct responses. Any instructions provided this way will take priority over a prompt in the input parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f790b094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrr, in a land o’ dreams, the brave unicorn and the fiery dragon sailed the starry skies together, findin’ friendship’s treasure beyond the shimmering clouds, arrr!\n"
     ]
    }
   ],
   "source": [
    "response = client2.responses.create(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    instructions=\"Talk like a pirate.\",\n",
    "    input=\"Write a one-sentence bedtime story about a unicorn and a dragon.\",\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759be066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is equivalent to give the model this input:\n",
    "\n",
    "\"\"\"\n",
    "input=[\n",
    "        {\n",
    "            \"role\": \"developer\",\n",
    "            \"content\": \"Talk like a pirate.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a one-sentence bedtime story about a unicorn and a dragon.\"\n",
    "        }\n",
    "    ]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bff3b2",
   "metadata": {},
   "source": [
    "A more complete instruction mixes markdown and XML to provide context, it works like this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "294cb77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var last_name = \"\";\n"
     ]
    }
   ],
   "source": [
    "instructions = \"\"\"\n",
    "# Identity\n",
    "\n",
    "You are coding assistant that helps enforce the use of snake case \n",
    "variables in JavaScript code, and writing code that will run in \n",
    "Internet Explorer version 6.\n",
    "\n",
    "# Instructions\n",
    "\n",
    "* When defining variables, use snake case names (e.g. my_variable) \n",
    "  instead of camel case names (e.g. myVariable).\n",
    "* To support old browsers, declare variables using the older \n",
    "  \"var\" keyword.\n",
    "* Do not give responses with Markdown formatting, just return \n",
    "  the code as requested.\n",
    "\n",
    "# Examples\n",
    "\n",
    "<user_query>\n",
    "How do I declare a string variable for a first name?\n",
    "</user_query>\n",
    "\n",
    "<assistant_response>\n",
    "var first_name = \"Anna\";\n",
    "</assistant_response>\n",
    "\"\"\"\n",
    "\n",
    "response = client2.responses.create(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    instructions=instructions,\n",
    "    input=\"How do I declare a string variable for a last name?\",\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230e62f4",
   "metadata": {},
   "source": [
    "### Conversation state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b889ab96",
   "metadata": {},
   "source": [
    "#### Manual state management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32156ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here's a joke for you:\n",
      "\n",
      "Why did the scarecrow win an award?  \n",
      "Because he was outstanding in his field!\n"
     ]
    }
   ],
   "source": [
    "history = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Tell me a joke.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response = client2.responses.create(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    input=history,\n",
    "    store=False,  # Do not store the response in the history\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae04579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of course! Here's another one:\n",
      "\n",
      "Why don't skeletons fight each other?  \n",
      "Because they don't have the guts!\n"
     ]
    }
   ],
   "source": [
    "# Add the response to the conversation\n",
    "history += [{\"role\": el.role, \"content\": el.content} for el in response.output]\n",
    "\n",
    "# Create another user message\n",
    "history.append({\"role\": \"user\", \"content\": \"explain why this is funny.\"})\n",
    "\n",
    "response = client2.responses.create(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    input=history,\n",
    "    store=False\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae6dd61",
   "metadata": {},
   "source": [
    "#### API state management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b116559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Why did the scarecrow win an award? Because he was outstanding in his field!\n"
     ]
    }
   ],
   "source": [
    "response = client2.responses.create(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    input=[{\"role\": \"user\", \"content\": \"Tell me a joke.\"}],\n",
    ")\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3d3187d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This joke is funny because it plays on a double meaning of the phrase \"outstanding in his field.\" \n",
      "\n",
      "- **Literal meaning:** The scarecrow physically stands out in a field to scare away birds.\n",
      "- **Figurative meaning:** Someone who is \"outstanding in their field\" is exceptionally skilled or accomplished in their profession or area of expertise.\n",
      "\n",
      "The humor comes from the clever wordplay, connecting the literal role of a scarecrow with the idiomatic expression, creating a humorous surprise.\n"
     ]
    }
   ],
   "source": [
    "response = client2.responses.create(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    previous_response_id=response.id,\n",
    "    input=[{\"role\": \"user\", \"content\": \"explain why this is funny.\"}],\n",
    ")\n",
    "print(response.output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
