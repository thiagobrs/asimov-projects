{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1a28d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fadef93",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()  # Load environment variables from .env file\n",
    "\n",
    "# Ensure the OPENAI_API_KEY is set\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if openai.api_key is None:\n",
    "    raise ValueError(\"OPENAI_API_KEY environment variable not set\")\n",
    "\n",
    "client = openai.OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e373bcd",
   "metadata": {},
   "source": [
    "# Model Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1523a390",
   "metadata": {},
   "source": [
    "## Evaluating model performance\n",
    "Test and improve model outputs through evaluations.\n",
    "\n",
    "Broadly, there are three steps to build and run evals for your LLM application.\n",
    "\n",
    "1) Describe the task to be done as an eval\n",
    "2) Run your eval with test inputs (a prompt and input data)\n",
    "3) Analyze the results, then iterate and improve on your prompt\n",
    "\n",
    "Here, we will configure evals programmatically using the Evals API. We can also configure evals in the OpenAI dashboard (easier)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcbcf38",
   "metadata": {},
   "source": [
    "### 1 - Create an eval for a task\n",
    "Let's say that we would like to use a model to classify the contents of IT support tickets into one of three categories: Hardware, Software, or Other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7214b411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hardware\n"
     ]
    }
   ],
   "source": [
    "# This is an example of our task\n",
    "\n",
    "instructions = \"\"\"\n",
    "You are an expert in categorizing IT support tickets. Given the support \n",
    "ticket below, categorize the request into one of \"Hardware\", \"Software\", \n",
    "or \"Other\". Respond with only one of those words.\n",
    "\"\"\"\n",
    "\n",
    "ticket = \"My monitor won't turn on - help!\"\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    input=[\n",
    "        {\"role\": \"developer\", \"content\": instructions},\n",
    "        {\"role\": \"user\", \"content\": ticket}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158433d5",
   "metadata": {},
   "source": [
    "Let's set up an eval to test this behavior via API. An eval needs two key ingredients:\n",
    "\n",
    "- **data_source_config**: A schema for the test data you will use along with the eval.\n",
    "- **testing_criteria**: The graders that determine if the model output is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e824be04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvalCreateResponse(id='eval_6844873742f08191b4fd05228d59f014', created_at=1749321527, data_source_config=EvalCustomDataSourceConfig(schema_={'type': 'object', 'properties': {'item': {'type': 'object', 'properties': {'ticket_text': {'type': 'string'}, 'correct_label': {'type': 'string'}}, 'required': ['ticket_text', 'correct_label']}, 'sample': {'type': 'object', 'properties': {'model': {'type': 'string'}, 'choices': {'type': 'array', 'items': {'type': 'object', 'properties': {'message': {'type': 'object', 'properties': {'role': {'type': 'string', 'enum': ['assistant']}, 'content': {'type': ['string', 'null']}, 'refusal': {'type': ['boolean', 'null']}, 'tool_calls': {'type': ['array', 'null'], 'items': {'type': 'object', 'properties': {'type': {'type': 'string', 'enum': ['function']}, 'function': {'type': 'object', 'properties': {'name': {'type': 'string'}, 'arguments': {'type': 'string'}}, 'required': ['name', 'arguments']}, 'id': {'type': 'string'}}, 'required': ['type', 'function', 'id']}}, 'function_call': {'type': ['object', 'null'], 'properties': {'name': {'type': 'string'}, 'arguments': {'type': 'string'}}, 'required': ['name', 'arguments']}}, 'required': ['role']}, 'finish_reason': {'type': 'string'}}, 'required': ['index', 'message', 'finish_reason']}}, 'output_text': {'type': 'string'}, 'output_json': {'type': 'object'}, 'output_tools': {'type': 'array', 'items': {'type': 'object'}}, 'input_tools': {'type': 'array', 'items': {'type': 'object'}}}, 'required': ['model', 'choices']}}, 'required': ['item', 'sample']}, type='custom'), metadata={}, name='IT Ticket Categorization', object='eval', testing_criteria=[StringCheckGrader(input='{{ sample.output_text }}', name='Match output to human label', operation='eq', reference='{{ item.correct_label }}', type='string_check', id='Match output to human label-4dc7c9eb-0859-493d-8a9a-e618acd35483')])\n"
     ]
    }
   ],
   "source": [
    "eval_obj = client.evals.create(\n",
    "    name=\"IT Ticket Categorization\",\n",
    "    data_source_config={\n",
    "        \"type\": \"custom\",\n",
    "        \"item_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"ticket_text\": {\"type\": \"string\"},\n",
    "                \"correct_label\": {\"type\": \"string\"},\n",
    "            },\n",
    "            \"required\": [\"ticket_text\", \"correct_label\"],\n",
    "        },\n",
    "        \"include_sample_schema\": True,\n",
    "    },\n",
    "    testing_criteria=[\n",
    "        {\n",
    "            \"type\": \"string_check\",\n",
    "            \"name\": \"Match output to human label\",\n",
    "            \"input\": \"{{ sample.output_text }}\",\n",
    "            \"operation\": \"eq\",\n",
    "            \"reference\": \"{{ item.correct_label }}\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(eval_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2dc0d2",
   "metadata": {},
   "source": [
    "### 2 - Test a prompt with our eval\n",
    "Now that we have defined how we want our app to behave in an eval, let's construct a prompt that reliably generates the correct output for a representative sample of test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88e107f",
   "metadata": {},
   "source": [
    "#### Uploading test data\n",
    "Let's upload our test data file to the OpenAI platform so we can reference it later. We can upload files in the OpenAI dashboard, but it's possible to upload files via API as well.\n",
    "\n",
    "After uploading the file, it is important to make note of the unique id property in the response payload (also available in the UI, if we had uploaded via the browser) - we will need to reference that value later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48bcc8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FileObject(id='file-2ECunj3WAhd8spBSDPhYfu', bytes=815, created_at=1749322200, filename='12_aula_tickets.jsonl', object='file', purpose='evals', status='processed', expires_at=None, status_details=None)\n"
     ]
    }
   ],
   "source": [
    "# Uploading test data\n",
    "\n",
    "file = client.files.create(\n",
    "    file=open(\"12_aula_tickets.jsonl\", \"rb\"),\n",
    "    purpose=\"evals\"\n",
    ")\n",
    "\n",
    "print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60826a62",
   "metadata": {},
   "source": [
    "#### Creating an eval run\n",
    "With our test data in place, let's evaluate a prompt and see how it performs against our test criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8a14a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunCreateResponse(id='evalrun_68448af3cda88191a5e22e1faccdc275', created_at=1749322483, data_source=DataSourceResponses(source=DataSourceResponsesSourceFileID(id='file-2ECunj3WAhd8spBSDPhYfu', type='file_id'), type='responses', input_messages=DataSourceResponsesInputMessagesTemplate(template=[DataSourceResponsesInputMessagesTemplateTemplateEvalItem(content=ResponseInputText(text=\"You are an expert in categorizing IT support tickets. Given the support ticket below, categorize the request into one of 'Hardware', 'Software', or 'Other'. Respond with only one of those words.\", type='input_text'), role='developer', type='message'), DataSourceResponsesInputMessagesTemplateTemplateEvalItem(content=ResponseInputText(text='{{ item.ticket_text }}', type='input_text'), role='user', type='message')], type='template'), model='gpt-4.1-nano', sampling_params=None), error=None, eval_id='eval_6844873742f08191b4fd05228d59f014', metadata={}, model='gpt-4.1-nano', name='Categorization text run', object='eval.run', per_model_usage=None, per_testing_criteria_results=None, report_url='https://platform.openai.com/evaluations/eval_6844873742f08191b4fd05228d59f014?project_id=proj_rWLJf3FdG6itdIq2XOiRg9LF&run_id=evalrun_68448af3cda88191a5e22e1faccdc275', result_counts=ResultCounts(errored=0, failed=0, passed=0, total=0), status='queued', shared_with_openai=False)\n"
     ]
    }
   ],
   "source": [
    "run = client.evals.runs.create(\n",
    "    \"eval_6844873742f08191b4fd05228d59f014\",\n",
    "    name=\"Categorization text run\",\n",
    "    data_source={\n",
    "        \"type\": \"responses\",\n",
    "        \"model\": \"gpt-4.1-nano\",\n",
    "        \"input_messages\": {\n",
    "            \"type\": \"template\",\n",
    "            \"template\": [\n",
    "                {\"role\": \"developer\", \"content\": \"You are an expert in categorizing IT support tickets. Given the support ticket below, categorize the request into one of 'Hardware', 'Software', or 'Other'. Respond with only one of those words.\"},\n",
    "                {\"role\": \"user\", \"content\": \"{{ item.ticket_text }}\"},\n",
    "            ],\n",
    "        },\n",
    "        \"source\": {\"type\": \"file_id\", \"id\": \"file-2ECunj3WAhd8spBSDPhYfu\"},\n",
    "    },\n",
    ")\n",
    "\n",
    "print(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f12a328",
   "metadata": {},
   "source": [
    "Our eval run has now been queued, and it will execute asynchronously as it processes every row in your data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b18212e",
   "metadata": {},
   "source": [
    "### 3 - Analyze the results\n",
    "Depending on the size of your dataset, the eval run may take some time to complete. You can view current status in the dashboard, but we can also fetch the current status of an eval run via API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8121917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunRetrieveResponse(id='evalrun_68448af3cda88191a5e22e1faccdc275', created_at=1749322483, data_source=DataSourceResponses(source=DataSourceResponsesSourceFileID(id='file-2ECunj3WAhd8spBSDPhYfu', type='file_id'), type='responses', input_messages=DataSourceResponsesInputMessagesTemplate(template=[DataSourceResponsesInputMessagesTemplateTemplateEvalItem(content=ResponseInputText(text=\"You are an expert in categorizing IT support tickets. Given the support ticket below, categorize the request into one of 'Hardware', 'Software', or 'Other'. Respond with only one of those words.\", type='input_text'), role='developer', type='message'), DataSourceResponsesInputMessagesTemplateTemplateEvalItem(content=ResponseInputText(text='{{ item.ticket_text }}', type='input_text'), role='user', type='message')], type='template'), model='gpt-4.1-nano', sampling_params=None), error=None, eval_id='eval_6844873742f08191b4fd05228d59f014', metadata={}, model='gpt-4.1-nano', name='Categorization text run', object='eval.run', per_model_usage=[PerModelUsage(cached_tokens=0, completion_tokens=18, invocation_count=9, run_model_name='gpt-4.1-nano', prompt_tokens=527, total_tokens=545, model_name='gpt-4.1-nano')], per_testing_criteria_results=[PerTestingCriteriaResult(failed=1, passed=8, testing_criteria='Match output to human label-4dc7c9eb-0859-493d-8a9a-e618acd35483')], report_url='https://platform.openai.com/evaluations/eval_6844873742f08191b4fd05228d59f014?project_id=proj_rWLJf3FdG6itdIq2XOiRg9LF&run_id=evalrun_68448af3cda88191a5e22e1faccdc275', result_counts=ResultCounts(errored=0, failed=1, passed=8, total=9), status='completed', shared_with_openai=False)\n"
     ]
    }
   ],
   "source": [
    "run = client.evals.runs.retrieve(eval_id=\"eval_6844873742f08191b4fd05228d59f014\", run_id=\"evalrun_68448af3cda88191a5e22e1faccdc275\")\n",
    "print(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918868f0",
   "metadata": {},
   "source": [
    "The API response contains granular information about test criteria results, API usage for generating model responses, and a report_url property that takes us to a page in the dashboard where we can explore the results visually."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
